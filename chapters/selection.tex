\section{Выбор исходных образцов и стратегий мутации}

При применении генеративного подхода решающую роль играет выбор образцов, которые станут источником для генерации, а также самих стратегий мутации. Например, применение битфлипов при фаззинге интерпретатора не даст пройти стадию корректно работающего лексического анализа, но в то время эта же стратегия может помочь обнаружить ошибки в парсере изображений. Для выбора исходных образцов необходимо выделение уникальных путей в программе и предпочтение мутации образцов, дающих наибольшее покрытие. При выборе стратегий мутации оказываются полезны некоторые методы решения задач о принятии решений в условии неопределённости, в частности для задачи о многоруких бандитах. Далее будут рассмотрены эти вопросы.

\subsection{Выбор образцов на основе покрытия}

Частой проблемой при фаззинге является медленное исследование отдалённых участков программы, связанное с тем, что фаззер не обладает информацией о том, каким путём может быть достижим тот или иной её участок и в следствие этого вынужден выбирать, какой образец мутировать случайным образом. Часто применяемой эвристикой для борьбы с этим является предпочтение тех образцов, которые имеют наибольшее покрытие программы в надежде, что их мутирование позволит продвинуться дальше.

Фаззеры формулируют пути в программе по-разному. Afl в частности вместо просто понимая покрытия как количества посещённых функций, базовых блоков или других структурных элементов рассматривает программу как конечный автомат и отслеживает переходы между его состояниями. При этом ещё и выполняется подсчёт каждого типа переходов и происходит разделение числа переходов на эмпирически подобранные классы эквивалентности, за счёт чего, с одной стороны, мы получаем, например, возможность отыскивать нередко приводящие к неожиданному поведению двойное исполнение блоков кода, и с другой стороны не проводим различий между примерами, при обработке которых используются многократно работающие циклы.

Аналогично afl предлагается использовать выделение классов эквивалентности, выделяя однократно, двукратно и многократно посещённые участки программы. Путь в программе представляется как словарь, ключами в котором являются адреса посещённых участков программы, а значениями -- классы эквивалентности. Данный подход, помимо прочего, позволяет убирать из программы точки останова после попадания адреса в класс многократно выполняющихся участков после выполнения трёх и более раз, за счёт чего влияние на скорость выполнения программы будет ограниченно числом точек останова и не будет постоянно возрастать, но в то же время мы по-прежнему имеем возможность узнать, был ли достигнут новый её участок.

Выбор кандидата на мутацию происходит с вероятностью, задаваемой простой формулой:
\begin{equation*}
	p_i = \frac{n_i}{\sum_{k=1}^{m} n_k},
	\vspace{5pt}
\end{equation*}

\noindent где $n_i$ это число уникальных точек в программе, посещённых при запуске $i$-го примера, а $p_i$ - вероятность выбора $i$-го примера.

\subsection{Задача о многоруких бандитах}

Задача о многоруких бандитах является одной из классических задач принятия решений в условиях неопределённости, а также обучении с подкреплением и находит применение в разных областях, например в медицине как потенциально более эффективный способ поиска лекарств или в рекламе для выбора наиболее подходящих рекламных баннеров \cite{bandits}. В своём классическом виде задача формулируется следующим образом: имеется $K$ случайных распределений с математическими ожиданиями $\mu_i$ и стандартными отклонениями $\sigma_i$, соответствующих игральным автоматам (отсюда название). На каждом ходу игрок выбирает один из автоматов и тянет за ручку, тем самым получая некоторый выигрыш, определяемый соответствующим распределением. Целью игрока является максимизация выигрыша. При этом параметры распределений изначально неизвестны, игроку доступна лишь оценка этих параметров на основании своих предыдущих выборов и наблюдавшихся результатов, перед игроком стоит выбор между использованием какого-то автомата, который он считает наилучшим, и исследованием остальных распределений в попытке собрать больше информации и потенциально найти более хороший вариант.

Если проводить параллели с генерацией данных для фаззинга, то вместо игровых автоматов мы выбираем среди набора стратегий мутации, а в качестве выигрыша рассматривается то, как много участков программы удалось покрыть сгенерированным при применении мутации образцом.

\subsection{Алгоритмы для задачи о многоруких бандитах}

Для измерения качества алгоритма как правило применяется метрика, отражающая общее количество недополученного выигрыша за $T$ шагов, формулируемая следующим образом:
\vspace{5pt}
\begin{equation*}
	R_T = T \mu^* - \sum_{t=1}^{T} \mu_j (t), \ \mathrm{где} \ \mu^* = \max_{i=1,...,K} \mu_i
	\vspace{7pt}
\end{equation*} 
\noindent то есть $\mu^*$ это математическое ожидание наилучшего распределения.

Часто применяемыми алгоритмами для задачи о многоруких бандитах являются $\epsilon$-жадный (epsilon-greedy) алгоритм и алгоритм Softmax. Помимо них есть и теоретически более эффективные, например, UCB (Upper Confidence Bound), но как показывают экспериментальные данные, простые эвристики показывают себя на уровне, а иногда и лучше. Две распространённые эвристики будут рассмотренны далее.

Epsilon-greedy является простой эвристикой, осуществляющей выбор между случайным исследованием и использованием наилучшего на данный момент варианта. Алгоритм выбора действий при использовании epsilon-greedy алгоритма можно описать следующим образом:
\begin{equation*}
	\vspace{5pt}
	\mathrm{действие\ на\ шаге}\ T+1 = \begin{cases}
		\argmax\limits_i \hat{\mu}_i(T) \ \mathrm{с\ вероятностью}\ 1-\epsilon\\
		\mathrm{случайное\ с\ вероятностью}\ \epsilon,
	\end{cases}
	\vspace{5pt}
\end{equation*}
где $\hat{\mu}_i(T)$ - оценки средних значений, накопленные за предыдущие $T$ шагов. Таким образом, данный алгоритм либо выполняет случайное исследование, либо выбирает тот из вариантов, который выглядит наиболее эффективным с точки зрения оценки среднего выигрыша.

Другой алгоритм, Softmax, в некотором роде обобщает epsilon-greedy. Для выбора варианта используется распределение Больцмана, формулируемое следующим образом:
\begin{equation*}
	\vspace{5pt}
	p_i(T+1) = \frac{e^{\hat{\mu}_i(T)/\tau}}{\sum_{j=1}^{K} e^{\hat{\mu}_j(T)/\tau}},
	\vspace{5pt}
\end{equation*}
где $p_i(T+1)$ -- вероятность выбора на $T+1$ шаге $i$-го варианта, $\hat{\mu}_i(T)$ -- оценки средних значений за предыдущие $T$ шагов, а $\tau$ -- температурный коэффициент. За счёт $\tau$ мы можем регулировать случайность алгоритма выбора. При стремлении $\tau$ к нулю алгоритм ведёт себя как простой жадный алгоритм, то есть всегда выбирается наиболее предпочтительный вариант, а при стремлении $\tau$ к бесконечности влияние средних значений снижается и выбор становится более случайным.

В данной работе выбор сделан в пользу softmax алгоритма и пользователю предоставлена возможность конфигурирования параметра $\tau$, поскольку он относительно прост для понимания и его настройка при необходимости не должна вызвать у пользователя серьёзных затруднений, но в то же время он показывает себя довольно хорошо на практике, и кроме того решает основную проблему в виде исключения неэффективных стратегий мутации.